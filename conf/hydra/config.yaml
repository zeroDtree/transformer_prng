defaults:
  - _self_

dataset:
  name: "mt19937-8"
  seed: 31

train:
  learning_rate: 5e-3
  lr_scheduler_type: linear
  train_batch_size: 16
  num_train_epochs: 150
  warmup_steps: 100
  weight_decay: 0.01
  seed: 31
  save_strategy: "no"
  eval_strategy: steps
  eval_steps: 500

model:
  name: "openai-community/gpt2"
  # name: "meta-llama/Meta-Llama-3.1-8B"
  # name: "meta-llama/Llama-2-7b-hf"
  # name: "HuggingFaceTB/SmolLM2-1.7B-Instruct"
  dtype: "bf16"

wandb:
  mode: offline
  project: "TCS-MT"
  group: "gpt2"

hydra:
  run:
    dir: hydra_outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  mode: RUN
  sweeper:
    params:
